{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f593bc-b076-49b5-b86f-63a7442cd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df40a7d6-3df4-4280-82b8-5805e9579cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import wandb\n",
    "from rnn_reserving.data_import import read_and_process_data\n",
    "from rnn_reserving.config import TrainingConfig, ModelConfig\n",
    "from rnn_reserving.dataloader import make_loaders\n",
    "\n",
    "from rnn_reserving.models import GRUInsurance\n",
    "\n",
    "from rnn_reserving.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bfd443-fe6c-4891-9941-e63d75808cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = TrainingConfig(\n",
    "    batch_size=32,\n",
    "    shuffle_train=True,\n",
    "    early_stopping_patience=10,\n",
    ")\n",
    "\n",
    "model_config = ModelConfig()\n",
    "target_cols = ['paid_loss_ratio']\n",
    "\n",
    "feature_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e0a3c5-2013-4075-afef-cac4a69a238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingConfig(data_path='./data/raw/ppauto_pos.csv', log_file='./logs/training.log', seed=42, device='cpu', model_name='gru_model', experiment_name='rnn_reserving_experiment', run_name='run_1', feature_cols=[], target_cols=['paid_loss_ratio'], epochs=100, batch_size=32, learning_rate=0.001, weight_decay=1e-05, shuffle_train=True, grad_clip=1.0, use_mixed_precision=True, scheduler_type='cosine', scheduler_patience=10, scheduler_factor=0.5, early_stopping_patience=10, early_stopping_min_delta=0.0001, checkpoint_dir='./checkpoints', save_best_only=True, save_frequency=5, log_interval=10, use_wandb=True, wandb_project='timeseries-forecasting', deterministic=False, num_workers=4)\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d59745-d0a1-4761-928c-769000f4cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = make_loaders(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3106ce83-bb85-43fe-b18c-4e53fcc7ae67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4098827c-4462-4866-a114-d4567795a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUInsurance(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08bc8254-e3f5-4d9e-a800-4bb95b67ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metrics_fn(output: torch.Tensor, target: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Compute metrics given model output and target.\"\"\"\n",
    "    mse = nn.MSELoss()(output, target).item()\n",
    "    mae = nn.L1Loss()(output, target).item()\n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d576a0e-dc8a-4326-94be-280364cfbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=50)  # 50 epochs cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dab48eac-4b32-4629-a264-9c2fa290d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: No netrc file found, creating one.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\TobyCook\\_netrc\n",
      "wandb: Currently logged in as: toby-cook (toby-cook-toby-cook) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\TobyCook\\Documents\\rnn_reserving\\notebooks\\wandb\\run-20251111_213723-28e8lkbl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting/runs/28e8lkbl' target=\"_blank\">gru_model_run</a></strong> to <a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting' target=\"_blank\">https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting/runs/28e8lkbl' target=\"_blank\">https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting/runs/28e8lkbl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimiser,\n",
    "    config,\n",
    "    scheduler=scheduler,\n",
    "    metrics_fn=metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "582177af-9bad-4047-a9ee-ca5a899f58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 21:37:56,455 - rnn_reserving.utils - INFO - Starting training on cpu\n",
      "2025-11-11 21:37:56,455 - rnn_reserving.utils - INFO - Starting training on cpu\n",
      "2025-11-11 21:37:56,455 - rnn_reserving.utils - INFO - Starting training on cpu\n",
      "2025-11-11 21:37:56,464 - rnn_reserving.utils - INFO - Config: {'data_path': './data/raw/ppauto_pos.csv', 'log_file': './logs/training.log', 'seed': 42, 'device': 'cpu', 'model_name': 'gru_model', 'experiment_name': 'rnn_reserving_experiment', 'run_name': 'run_1', 'feature_cols': [], 'target_cols': ['paid_loss_ratio'], 'epochs': 100, 'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'shuffle_train': True, 'grad_clip': 1.0, 'use_mixed_precision': True, 'scheduler_type': 'cosine', 'scheduler_patience': 10, 'scheduler_factor': 0.5, 'early_stopping_patience': 10, 'early_stopping_min_delta': 0.0001, 'checkpoint_dir': './checkpoints', 'save_best_only': True, 'save_frequency': 5, 'log_interval': 10, 'use_wandb': True, 'wandb_project': 'timeseries-forecasting', 'deterministic': False, 'num_workers': 4}\n",
      "2025-11-11 21:37:56,464 - rnn_reserving.utils - INFO - Config: {'data_path': './data/raw/ppauto_pos.csv', 'log_file': './logs/training.log', 'seed': 42, 'device': 'cpu', 'model_name': 'gru_model', 'experiment_name': 'rnn_reserving_experiment', 'run_name': 'run_1', 'feature_cols': [], 'target_cols': ['paid_loss_ratio'], 'epochs': 100, 'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'shuffle_train': True, 'grad_clip': 1.0, 'use_mixed_precision': True, 'scheduler_type': 'cosine', 'scheduler_patience': 10, 'scheduler_factor': 0.5, 'early_stopping_patience': 10, 'early_stopping_min_delta': 0.0001, 'checkpoint_dir': './checkpoints', 'save_best_only': True, 'save_frequency': 5, 'log_interval': 10, 'use_wandb': True, 'wandb_project': 'timeseries-forecasting', 'deterministic': False, 'num_workers': 4}\n",
      "2025-11-11 21:37:56,464 - rnn_reserving.utils - INFO - Config: {'data_path': './data/raw/ppauto_pos.csv', 'log_file': './logs/training.log', 'seed': 42, 'device': 'cpu', 'model_name': 'gru_model', 'experiment_name': 'rnn_reserving_experiment', 'run_name': 'run_1', 'feature_cols': [], 'target_cols': ['paid_loss_ratio'], 'epochs': 100, 'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 1e-05, 'shuffle_train': True, 'grad_clip': 1.0, 'use_mixed_precision': True, 'scheduler_type': 'cosine', 'scheduler_patience': 10, 'scheduler_factor': 0.5, 'early_stopping_patience': 10, 'early_stopping_min_delta': 0.0001, 'checkpoint_dir': './checkpoints', 'save_best_only': True, 'save_frequency': 5, 'log_interval': 10, 'use_wandb': True, 'wandb_project': 'timeseries-forecasting', 'deterministic': False, 'num_workers': 4}\n",
      "Epoch 1/100 [Train]:   0%|                                                                                             | 0/32 [00:21<?, ?it/s]\n",
      "2025-11-11 21:38:18,214 - rnn_reserving.utils - ERROR - Training failed with error: too many values to unpack (expected 2, got 3)\n",
      "2025-11-11 21:38:18,214 - rnn_reserving.utils - ERROR - Training failed with error: too many values to unpack (expected 2, got 3)\n",
      "2025-11-11 21:38:18,214 - rnn_reserving.utils - ERROR - Training failed with error: too many values to unpack (expected 2, got 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gru_model_run</strong> at: <a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting/runs/28e8lkbl' target=\"_blank\">https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting/runs/28e8lkbl</a><br> View project at: <a href='https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting' target=\"_blank\">https://wandb.ai/toby-cook-toby-cook/timeseries-forecasting</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_213723-28e8lkbl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\rnn_reserving\\src\\rnn_reserving\\trainer.py:235\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.start_epoch + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.config.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    234\u001b[39m         \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m         \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    238\u001b[39m         val_metrics = \u001b[38;5;28mself\u001b[39m.validate(epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\rnn_reserving\\src\\rnn_reserving\\trainer.py:152\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    145\u001b[39m metrics_tracker = MetricsTracker()\n\u001b[32m    147\u001b[39m pbar = tqdm(\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_loader,\n\u001b[32m    149\u001b[39m     desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Train]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    150\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[32m    153\u001b[39m     data, target = data.to(\u001b[38;5;28mself\u001b[39m.device), target.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2, got 3)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
